{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58629e9b-b269-4c05-a560-edb98b1f5f4b",
   "metadata": {},
   "source": [
    "### Setup Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abfb42b-c24b-436b-a854-f6849f9c3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import Image, display, HTML\n",
    "\n",
    "root = 'https://api.weburban.com/'\n",
    "apiKey = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba24a5-660d-450a-8361-43e00ae2f528",
   "metadata": {},
   "source": [
    "### Image k-Means analysis endpoint\n",
    "Colours are extracted using k-means analysis and output as RGB values, with their frequency.\\\n",
    "`k` : the number of colours extracted\\\n",
    "`imageUrl` : the url for the image to be analysed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ac1b7-258a-4823-b760-bccd56e1d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_analysis_endpoint = 'image/kmeans-palette'\n",
    "imageUrl = 'https://weburban-uploads-raw.s3.ap-southeast-2.amazonaws.com/poster.jpg'\n",
    "\n",
    "h = {\n",
    "    'x-api-key': apiKey\n",
    "}\n",
    "\n",
    "json_request = { \n",
    "    'k': 16, \n",
    "    'imageUrl': imageUrl\n",
    "}\n",
    "\n",
    "response = requests.post(root + image_analysis_endpoint, json=json_request, headers = h)\n",
    "output = response.json()\n",
    "Image(url = imageUrl, width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce321ec8-5bb2-4c74-a746-46ae1fac621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['Output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3cfed-ec5e-411f-8d38-83c05aea29ce",
   "metadata": {},
   "source": [
    "Here is a simple execution of html code styling of the RGB info returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e227f3-7e0b-49cf-899d-5e2ec5b29731",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = \"\"\n",
    "for color in output['Output']:\n",
    "    html_string += '<div style=\\\"float:left;height:100px;width:50px;background-color:rgba({0},{1},{2});\\\"\"></div>'.format(\n",
    "        color['RGB']['R'],\n",
    "        color['RGB']['G'],\n",
    "        color['RGB']['B']\n",
    "    )\n",
    "    \n",
    "display( HTML(html_string) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2e70c-d049-4882-98a8-db6025c9a7d2",
   "metadata": {},
   "source": [
    "### Image to text recognition analysis endpoint\n",
    "Text is extracted from an image with a response as a simple list of text or as complex as text and positional data.\\\n",
    "`response` : either `simple` or `complex`\\\n",
    "`imageUrl` : the url for the image to be analysed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb780986-64b8-46b9-98b8-b6d67252646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_analysis_endpoint = 'image/to-text'\n",
    "imageUrl = 'https://weburban-uploads-raw.s3.ap-southeast-2.amazonaws.com/poster.jpg'\n",
    "\n",
    "h = {\n",
    "    'x-api-key': apiKey\n",
    "}\n",
    "\n",
    "json_request = { \n",
    "    'response': 'simple', \n",
    "    'imageUrl': imageUrl\n",
    "}\n",
    "\n",
    "response = requests.post(root + image_analysis_endpoint, json=json_request, headers = h)\n",
    "output = response.json()\n",
    "Image(url = imageUrl, width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e721f5-8b13-4db0-997c-6c792844b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb814e3-d2c1-4279-99d6-04a7b4de40d4",
   "metadata": {},
   "source": [
    "### Measure text similarity using Hashing Vectorizer for text embeddings\n",
    "`text1` is a paragraph of text, and `text2` is a different paragraph. You can measure their similarity using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6b7ca-0ccb-4b76-b9ec-9c118275c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_analysis_endpoint = 'text/to-vector'\n",
    "text1 = 'This guide describes best practices in using eye tracking technology for research in a variety of disciplines. A basic outline of the anatomy and physiology of the eyes and of eye movements is provided, along with a description of the sorts of research questions eye tracking can address. We then explain how eye tracking technology works and what sorts of data it generates, and provide guidance on how to select and use an eye tracker as well as selecting appropriate eye tracking measures. Challenges to the validity of eye tracking studies are described, along with recommendations for overcoming these challenges. We then outline correct reporting standards for eye tracking studies.'\n",
    "text2 = 'Participants following spoken instructions to touch or move either real objects or objects on a computer screen make saccadic eye movements (to the objects) that are closely time-locked to relevant information in the speech stream. Monitoring eye movements using a head-mounted eye-camera allows one to use the locations and latencies of fixations to examine spoken word recognition during continuous speech in natural contexts. Preliminary work using this paradigm provides striking evidence for the continuous and incremental nature of comprehension, as well as clear effects of visual context on the earliest moments of linguistic processing. We review the eye-movement paradigm and refer to recent experiments applying the paradigm to issues of spoken word recognition (e.g. lexical competitor effects), syntactic processing (e.g. the interaction of referential context and ambiguity resolution), reference resolution (disambiguating temporarily ambiguous referential phrases), focus (modulating the salience of certain objects via contrastive stress), as well as issues in cross-modality integration that are central to evaluating the modularity hypothesis.'\n",
    "n_features = 100\n",
    "\n",
    "h = {\n",
    "    'x-api-key': apiKey\n",
    "}\n",
    "\n",
    "json_request = { \n",
    "    'n_features': n_features, \n",
    "    'text1': text1,\n",
    "    'text2': text2\n",
    "}\n",
    "\n",
    "response = requests.post(root + image_analysis_endpoint, json=json_request, headers = h)\n",
    "output = response.json()\n",
    "\n",
    "# bray curtis dissimilarity between text1 and text2\n",
    "output['Output']['braycurtis_dissimilarity']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
